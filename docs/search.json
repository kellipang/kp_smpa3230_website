[
  {
    "objectID": "academicpaper.html",
    "href": "academicpaper.html",
    "title": "Academic Paper",
    "section": "",
    "text": "The History and Future of OTC Derivatives\nFrom the trading of clay tokens in ancient Mesopotamia to Japan’s Dojima Rice Exchange in the 1730s, various forms of financial derivatives have appeared throughout human history. Like many other financial processes, modern-day derivative trading is primarily electronic. While today’s trading methods look far different from historical practices, the primary purpose of financial derivatives remains the same. Derivatives are financial securities “whose value depends on (or derives from) the value of an underlying asset, reference rate or index” (Salifu). Through these financial contracts, two parties can bet on future changes in an underlying asset. A derivative’s underlying asset can be nearly anything– even another derivative. Traders use popular derivatives like forwards, futures contracts, and credit default swaps, to efficiently transfer risk between participating parties. There are two different ways to trade derivatives: through regulated exchanges or over-the-counter.\nIn this paper, I will be focusing on the history of the unregulated, over-the-counter derivatives market, specifically Brooksley Born’s 1998 concept release, and how OTC derivatives contributed to the 2008 financial crisis. Following my analysis, I will propose solutions to issues regarding transparency and systemic risk in the OTC market.\n\nWhat are Over-the-Counter Derivatives?\nUnlike traditional derivatives, OTC trading happens in a decentralized market and does not involve clearing organizations. While this allows OTC derivatives to be highly customizable and fit parties’ specific preferences, the lack of regulation exposes traders to counterparty risk. Since OTC trades are not subject to clearinghouse rules, parties are granted more privacy at the cost of public transparency. Critics of OTC derivatives cite systemic risk and lack of transparency as the most concerning issues.\nIn addressing the Senate Banking Committee in March 2002, former Federal Reserve Chairman Alan Greenspan called OTC derivatives, “a major contributor to the flexibility and resiliency of our financial system” (Salifu). When institutions and individuals redistribute their risk via OTC trading, they are “able to lower their cost of capital, manage their credit exposures, and increase their competitiveness both in the United States and the rest of the world” (Salifu). Because OTC derivatives are inherently reliant on other assets, their widespread use has allowed for a complex, interconnectivity to develop between different financial institutions and their respective markets. Under proper precautions, OTC derivatives are powerful financial instruments in managing business risk.\n\n\nThe U.S. Commodity Futures Trading Commission 1998 Concept Release\nThe U.S. Commodity Futures Trading Commission, created by Congress in 1974, is an independent federal agency that regulates U.S. commodity futures contracts. Although OTC derivatives and futures contracts share many similar characteristics, Congress and the CFTC “worked to create legal certainty around the notion that OTC derivatives were not futures “ (Rose). This was done under pressure from the financial industry to promote growth in the OTC market. Financiers encouraged the market’s rapid development because they believed it was indicative of a strong economy. Congress and the CFTC officially defined OTC derivatives as tailor-made securities between two sophisticated parties not containing an exchange or requiring counterparty creditworthiness; however, they never clarified whether or not the derivatives were legally futures (Rose). Because this clarification was not made, there was confusion regarding whether or not OTC derivatives fell under the CFTC’s regulatory domain.\nBrooksley Born was appointed as chairwoman and commissioner of the CFTC in August 1996 by President Bill Clinton. A former “lawyer with a track record for activist causes”, Born was known for being passionately progressive, especially in comparison to some of her “absolutist” colleagues (Schmitt). At the time, Greenspan was a firm believer that the markets would correct themselves. The lack of regulation in place reflected this belief and was largely supported by other financial leaders.\nFrom 1992 to 1997, the OTC market quintupled in value from $5.3 to $29 trillion (Rose). Concerned by the accelerated growth in the OTC market, Born directed the CFTC to compile information about the market in 1997 to determine “whether the terms and conditions of existing exemptions for OTC derivatives needed adjustment” (Rose). In 1998, Born shared the Concept Release, a general set of questions asking for public insight on the inner workings of the market and regulation of OTC derivatives.\nDespite intending for the Concept Release to simply gather public opinion and gain a better understanding of the OTC market, Born and her team received great backlash for even insinuating that the market may require oversight. Greenspan, along with Arthur Levitt, the Securities and Exchange Commission Chairman, and Robert Rubin, Treasury Secretary, promptly released a joint statement after the Concept Release’s publication and followed up with a letter to Congress a month later. In both, Greenspan, Levitt, and Rubin brought forth their concerns over an increase in the legal uncertainty of the derivative market and rejected the idea that OTCs fell under the CFTC’s jurisdiction. The trio proposed legislation to install oversight on the CFTC and prevent the agency from taking any further action over the OTC derivative market, resulting in H.R. 4062, or the Financial Derivatives Supervisory Improvement Act.\nThe Financials Derivatives Supervisory Improvement Act of 1998 advanced the idea that the Working Group on Financial Derivatives must study current regulations on derivative markets, recommend changes to modernize the regulations, and report their findings to Congress. Additionally, the act barred the CFTC from proposing policy or releasing statements without the Treasury Secretary’s approval. Because the Working Group on Financial Derivatives is composed of the Treasury of Secretary, and Chairmen of the Federal Reserve, SEC, and CFTC, Greenspan, Levitt, and Rubin designed the legislation to outnumber Born. The passing of H.R. 4062 would effectively strip the power to publicly question and discuss potential changes to the OTC market from Born and the CFTC.\nGreenspan defended his belief in free markets, exclaiming that “The primary source of regulatory effectiveness has always been private traders being knowledgeable of their counterparties” (Greenspan). In Greenspan’s eyes, high credit ratings, the threat of legal fines, and care for reputation were enough to protect the market from major losses. Of course, Greenspan was unable to predict the rise in faulty credit rating and subsequent increase in subprime mortgages that would happen within the next decade. For Born to doubt the market’s ability to self-correct and suggest regulation, at the time, was absurd because Greenspan believed that “Government regulation can only act as a backup” (Greenspan).\nBefore ending her own testimony, Born cautioned Congress that “Losses resulting from the misuse of OTC derivatives instruments or from sales practice abuses in the OTC derivatives market can affect many Americans” (Born). Because of the interconnected and complex nature of OTC derivatives, losses in the OTC derivative market inevitably have a widespread impact. Nevertheless, Congressional leaders disregarded Born and the CFTC, listened to Greenspan, and implemented H.R. 4062. In late 2000, Congress implemented the Commodity Futures Modernization Act which further ensured the OTC derivative market would not be regulated. By placing full trust in financial professionals and the market’s ability to self-correct, regulatory leaders led the world into the worst financial crisis since the Great Depression.\n\n\nOver-the-Counter Derivatives and the Great Recession\nAs Born predicted, the OTC derivatives market suffered major losses in 2008, and in turn, affected many Americans. OTC derivatives, specifically in the form of credit default swaps, played a prime role in intensifying the impact of the Great Recession.\nDesigned in the 1990s to help banks reduce their risk, credit default swaps are a type of OTC derivative where a buyer pays the seller to protect them in case of a credit event. For example, if a firm faces bankruptcy, fails to pay, or reorganizes and can no longer fulfill its contract with an investor, an investor who bought a credit default swap would be compensated by the seller of the swap. Credit default swaps are similar to credit insurance, however, because they are classified as OTC derivatives they are not subject to state insurance regulations. Without insurance or OTC regulations, “CDS dealers had no regulatory obligation to hold capital against the exposures created by their CDS transactions” (Rose). Since they are not required to hold a minimum amount of capital, CDS sellers were able to create as many credit default swaps as they wanted to without limit. By 2008, credit default swaps “accounted for nearly 10% of OTC derivatives outstanding” and were worth “something close to $57 trillion in notional value” (Rose). As put by Eric Dinallo, former Insurance Superintendent of New York, “swaps came to be used not to reduce risk, but to create or assume it” (Dinallo).\nThe fall of American International Group in September 2008 highlights how the lack of regulation in credit default swaps and the OTC market directly intensified the effects of the recession. Going into the third quarter of 2008, AIG had credit fault swaps on approximately $500 billion worth of assets, $78 billion of which were on multi-sector collateralized debt obligations. Multi-sector collateralized debt obligations are complicated securities “backed by debt payments from residential and commercial mortgages, home equity loans, and more” (). As the subprime mortgage industry collapsed with the housing bubble in 2007, AIG had to start paying back many of their CDS buyers. By July 2008, AIG had to pay approximately $16.5 billion to its CDS counterparties (Rose). Because credit rating agencies were concerned about AIG’s capital, they lowered the company’s debt ratings. In response, AIG’s CDS buyers called for $14.5 billion of collateral, sending AIG into a large and immediate liquidity crisis (Rose). AIG’s descent put the world’s financial system at risk because of the interconnected, complex nature of the financial industry, and the fact that many global firms had bought credit default swaps from AIG. If AIG had failed, many firms would have defaulted on it. Luckily, however, the United States government bailed the company out with a $180 billion loan in exchange for majority ownership. The near-fall and bailout of AIG is an undeniably prominent event in the Great Recession. In reflection on the recession, Born noted that “Recognizing the dangers . . . was not rocket science, but it was contrary to the conventional wisdom and certainly contrary to the economic interests of Wall Street at the moment” (Rose). Had financial regulators been more willing to listen to Born’s concerns in the 90s, the consequences of the Great Recession could have been greatly mitigated.\n\n\nThe Future of Over-the-Counter Derivatives and Potential Solutions\nAt the Pittsburgh summit in 2009, the G20 nations met to discuss potential methods for oversight of the OTC derivative market. In the wake of the recession, leaders opted for a slow rollout of OTC requirements, agreeing that all standardized OTC derivatives get traded via central counterparty and on electronic platforms, as well as be reported to a central trade repository. The goal of these changes was to minimize counterparty risk and increase market transparency.\nThe United States enacted the Dodd-Frank Wall Street Reform and Consumer Protection Act on July 21, 2010, to implement the regulations addressed at the G20 Pittsburgh summit. Title VII of the act provides a regulatory guide to the OTC market by dividing regulatory power between the SEC and CFTC. Since the act was signed, the agencies have worked together to propose many rules within the Dodd-Frank guidelines.\nOne solution discussed both at the G20 summit and in the Dodd-Frank Act is the implementation of central counterparties. CCPs are separate legal entities that split the initial contract of two counterparties into two so that the counterparties can adopt the CCP as their new counterparty. Three benefits of CCP use are multilateral netting, simplified risk management, and public transparency (Cecchetti et al.). Multilateral netting refers to the summation of an individual firm’s bilateral netting and effectively reduces risk and exposure (Cecchetti et al.). While CCPs have demonstrated clear benefits within the past decade of minimizing one issue with current OTC regulations, only standardized OTC derivatives are required to utilize CCPs. Custom OTC derivatives should also be encouraged to use CCPs to minimize the most risk within the market.\nAs OTC derivatives evolve in complexity and the market continues to develop innovative solutions– whether that be additional clearing houses or some other regulation– are necessary to find a balance between growth and stability. Regulators and leaders must be open to differing opinions, recognizing that while the marketplace may appear to be thriving in its present state, long-term sustainability is not always guaranteed."
  },
  {
    "objectID": "paper.html",
    "href": "paper.html",
    "title": "Policy Brief",
    "section": "",
    "text": "The Effects of Bisphenol A on Human Health\n\nExecutive Summary\nBisphenol A, BPA, is a chemical compound used to create polycarbonate plastics and epoxy resin. People are exposed to BPA when they eat and drink from containers made/lined with BPA plastics or resins. Researchers have found that BPA has harmful effects on human development and fertility, and may cause obesity, type II diabetes, high blood pressure, and autoimmune disorders. Recent chemical assessments by the European Food Safety Authority indicate that even low exposure to BPA can result in long-term health issues. In accordance with the EFSA’s findings, the U.S. Food and Drug Administration should review current safety standards surrounding BPA exposure. Further, the FDA should consider implementing a program to periodically review the safety of chemical compounds.\n\n\nIntroduction\nIn 1891, Russian chemist Aleksandr P. Dianin created bisphenol A for the first time. One hundred thirty-one years later, the chemical compound can be found in can linings, water bottles, and the bloodstreams of 93% of Americans over the age of six. How are so many people exposed to BPA? And perhaps more importantly, what does BPA exposure mean for people’s health?\nBisphenol A, typically referred to as BPA, has been a key component in epoxy resin and polycarbonate plastic production since the 1950s. Known for their resilience and strength, epoxy resin and polycarbonate plastics are used in everything from aircraft manufacturing to thermal paper receipts and dental sealants to food and beverage packaging. People are most commonly exposed to BPA when the chemical leaches from packaging liners to their foods and drinks. While humans generally process small amounts of BPA quickly, scientists have found that overexposure- also stemming from nonfood sources and the accumulation of BPA in body fat- can lead to a myriad of health problems.\nBecause it has a similar chemical structure to estrogen, BPA is known as an endocrine-disrupting compound. Human endocrine systems are in charge of regulating the hormones that control the body’s growth, development, metabolism, and reproduction. BPA interferes with people’s endocrine systems by intercepting interactions between the body’s hormones and hormone receptors, exaggerating the effects of natural hormones. Therefore, BPA has been linked to problems like infertility and early puberty, as well as cases of breast and prostate cancer, and polycystic ovary syndrome. Research has also found that prenatal exposure to BPA has a significant impact on brain development, resulting in future behavioral problems. Beyond affecting the endocrine system, BPA can be linked to obesity, type II diabetes, high blood pressure, and autoimmune disorders.\n\n\nApproaches and Results\nInternational organizations are well aware of the health implications of BPA exposure and have established policies to limit BPA’s use since 2008. After completing a chemical risk assessment, Health Canada officials proposed banning BPA’s use in plastic baby bottles and infant formula cans. The EFSA and China’s Ministry of Health made similar proposals in 2011, and the FDA followed suit in 2012.\nThe FDA last updated its stance on BPA in June 2014, when the FDA’s BPA Joint Emerging Science Working Group reviewed a series of toxicological and scientific studies. In their assessment, the FDA declared that the no-observed-adverse-effect-level would remain at five milligrams per kilogram of body weight per day. Anything below the NOAEL is presumably a safe amount of exposure. The FDA derived the 5 mg limit from a 2008 review conducted by the National Toxicology Program’s Center for the Evaluation of Risks to Human Reproduction that analyzed multigenerational rodent studies. Key takeaways from the review included “minimal concern” for BPA’s effects on the mammary gland and “some concern” for the brain, behavior, and prostate gland development. According to chemists from the Division of Food Contact Notifications, Division of Biotechnology, and Generally Recognized as Safe Notice Review, the average American child, age two or younger is exposed to 0.0002 mg/kg-bw of BPA per day. Meanwhile, the members of the U.S. population over the age of two are exposed to an average of 0.0005 mg/kg-bw/day. When compared to the NTP-CERHR research and the FDA’s NOAEL, Americans’ average BPA consumption appears healthy across age ranges.\nIn December 2021, the EFSA released a draft calling for the tolerable daily intake of BPA to be reduced from 4 micrograms to 0.04 nanograms. The EFSA’s proposed limit is 100,000 times less than its former standard. A nanogram is one-billionth of a gram. A grain of salt weighs about 58,500 nanograms. Therefore, setting the human BPA limit to 0.04 nanograms requires the complete removal of BPA from nearly every part of the food and beverage supply chain.\nHistorically, chemical assessments have been based on larger research projects and toxicology reviews that connect increased exposure to higher risks of disease. The EFSA reached the new safe exposure level by including smaller research studies that tracked how low levels of exposure may lead to small changes and eventually create larger health problems in their review. For example, a 2016 study conducted by China’s Anhui Medical University showed that mice developed more Th17 cells when exposed to 0.04 nanograms of BPA both before and after their birth. Excessive amounts of Th17 cells are known for causing inflammatory autoimmune systems like multiple sclerosis and rheumatoid arthritis. By considering more than just large toxicology studies, the EFSA has taken a more holistic approach to understanding the effects BPA has on human health. Consequently, the organization has also set a new approach for the reexamination of other chemical compounds.\n\n\nConclusion\nWhile BPA has allowed for many advancements in consumer and industrial products, years of research show that overexposure leads to increased endocrine, cardiovascular, and autoimmune health risks. The most recent research highlights how even minimal exposure can result in lasting health issues. As more research is conducted on the relationship between BPA and human health, policies must adjust to minimize people’s exposure to the chemical.\n\n\nImplications and Recommendations\nWith the emergence of the EFSA’s new data, grassroots organizations and doctors filed a petition with the FDA. On January 27, 2022, the Environmental Defense Fund, in partnership with Breast Cancer Prevention Partners, Clean Water Action, Consumer Reports, Endocrine Society, Environmental Working Group, Healthy Babies Bright Futures, Dr. Marciel Maffini, and Dr. Linda Birnbaum submitted a petition to the FDA, calling for the organization to remove and restrict BPA approvals.\nCurrently, the average American’s daily exposure to BPA is more than 5,000 times higher than the EFSA’s new proposed safety level. As put by petition signee and Consumer Reports’ Director of Food Policy, Brian Ronholm, “When you factor in the science, it’s pretty clear that we need to have a regulatory approach that significantly reduces consumer exposure.”\nBeyond reconsidering safe exposure levels of BPA, the FDA should implement a formal program to periodically review decisions about chemical exposures and food safety. According to Dr. Maricel Maffini, who co-authored the petition and has spent the last 20 years studying endocrine systems and chemical safety, “If we have a system, we shouldn’t have to look at regulations anti anything […] after X number of years you just review them based on new hazard information.”\nLowering the BPA safe exposure level is just the beginning of minimizing human health risks that come from our food and drink supply. More research surrounding BPA and other chemical compounds must be conducted to understand the effects of toxicity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kelli Pang",
    "section": "",
    "text": "Education\nI am a fourth-year student at the George Washington University, studying business with concentrations in business analytics and marketing, and a minor in journalism and mass communications.\n\n\nProfessional Experience\nI have been an ad operations intern at Industry Dive since June 2022. Previously, I worked at the Sports and Fitness Industry Association as a marketing and communications intern."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website is my final project for SMPA3230, Reporting in the Digital Age."
  },
  {
    "objectID": "va_elections.html",
    "href": "va_elections.html",
    "title": "Virginia Election",
    "section": "",
    "text": "The data used can be seen, downloaded, and filtered in the first table below. The original data used to create this dataframe was collected from virginia.gov. The base dataset can be downloaded from the table.\n\n#These are the R packages that need to be loaded to run the following exercises. \nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.5      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nOpen the data frame using read_excel and assign it to the variable va_df. Using the mutate() function and as.factor(), we can change the locality column’s data type from character to factor. This will allow us to create a drop down selection to filter the following table by county.\n\nva_df <- read_excel(\"va_2022_election_results.xlsx\")\nva_df <-  va_df %>% mutate(locality = as.factor(locality))\n\n\nCreating the Table\nThe following code snippet will create an interactive, paginated data table of the data set saved under va_df. The code below uses datatable() from the package DT to build an interactive table of the va_df data. The buttons extensions allows viewers to copy and download the data as their preferred file format.\n\nDT::datatable(va_df,\n              filter = \"top\", \n              extensions = 'Buttons', \n              options = list(   \n                dom = 'Bfrtip',\n                buttons = c('copy', 'csv', \"excel\")\n              )) \n\n\n\n\n\n\n\n\nExercise 1\nIn the next chunk of code, we will look at the counties with the greatest difference between the percent of voters for Youngkin and the percent of voters for Trump. First we have to add a new column to the va_Df data frame using the function mutate(). For each county/row, we will subtract the percent of Trump voters from the percent of Youngkin voters and save the value to a new column called diff_pct_vote_r. Next, we create a subset of va_df and save it under top5_diff_r. The subset is created by arranging the percent difference between Youngkin and Trump voters from highest to lowest and taking the top 5 values. Using the new dataframe, top5_diff_r, we will make a barchart with the ggplot package. ggtitle(), xlab(), and ylab() label different parts of the final graph.\n\nva_df <- va_df %>% \n  mutate(\n    diff_pct_vote_r = pct_youngkin - trump_pct\n  )\n\ntop5_diff_r <- va_df %>% \n               arrange(desc(diff_pct_vote_r)) %>% \n               head(5)\n\nggplot(top5_diff_r, aes(y=locality, x=diff_pct_vote_r))+\n  geom_col()+\n  ggtitle(\"Top 5 Counties with the Largest Difference Between the % \\nof Votes Received by Youngkin and Trump\")+\n  xlab(\"% Difference between Votes for Youngkin and Trump\")+\n  ylab(\"County\")\n\n\n\n\n\n\nExercise 2\nIn the next chunk of code, we will create a chart that shows the 5 counties where Youngkin got the highest percent of votes. The first step is to create a subset of va_df that only shows the 5 counties where Youngkin got the highest percent of votes. We will nest desc() in arrange() to sort the data, and then take the first five counties with head(), and save the results to top5_youngkin. We can now graph top5_youngkin using the ggplot package. ggtitle(), xlab(), and ylab() label different parts of the final graph.\n\ntop5_youngkin <- va_df %>% \n                 arrange(desc(pct_youngkin)) %>% \n                 head(5)\n\nggplot(top5_youngkin, aes(x=locality, y=pct_youngkin))+\n  geom_col()+\n  ggtitle(\"Five Counties with the Highest % of Voters for Youngkin for VA Governor\")+\n  xlab(\"County\")+\n  ylab(\"% of Voters\")\n\n\n\n\n\n\nExercise 3\nThe following code walks through how to create a table that shows the five counties where McAuliffe got the highest percent of votes for governor. The first step is to create a subset of va_df that only shows the 5 counties where McAuliffe got the highest percent of votes. We will select the locality and pct_mcauliffe columns, then nest desc() in arrange() to sort the data from highest to lowest, and then take the first five counties with head(), and save the results to top5_mcauliffe. We then create a table from the subset of data using the datatable function from the DT package. To make the table simple, we remove row names, searching, and paging by setting the arguements to FALSE.\n\ntop5_mcauliffe <- va_df %>% \n                 select(locality, pct_mcauliffe) %>% \n                 arrange(desc(pct_mcauliffe)) %>% \n                 head(5) \n\ntop5_mcauliffe_table <- DT::datatable(top5_mcauliffe, \n                        rownames = FALSE, \n                        options = list(searching = FALSE, paging = FALSE, dom = \"tip\")) \n\ntop5_mcauliffe_table\n\n\n\n\n\n\n\n\nExercise 4\nThe five localities with the largest percent difference between votes for Biden and votes for McAuliffe are Charlotte, Emporia, Mecklenburg, Radford, and Richmond.We can reach this insight and the following graph by first adding another column to the original dataset. the diff_pct_vote_d column is calculated by subtracting pct_mcauliffe from biden_pct. We add the column to the data frame by using the mutate() function. The following code takes only the locality, biden_pct, pct_mcauliffe, and new diff_pct_vote_d column from the va_df data, and sorts the rows by diff_pct_vote_d from highest to lowest. head(5) means only the top 5 rows are saved to the data subset top5_diff_d. We can now make a graph of the five localities with the biggest percent difference between Biden and McAuliffe voters using ggplot(). ggtitle(), xlab(), and ylab() label different parts of the final graph.\n\nva_df <- va_df %>% \n  mutate(\n    diff_pct_vote_d = biden_pct - pct_mcauliffe\n  )\n\ntop5_diff_d <- va_df %>% \n               select(locality, biden_pct, pct_mcauliffe, diff_pct_vote_d) %>% \n               arrange(desc(diff_pct_vote_d)) %>% \n               head(5)\n\nggplot(top5_diff_d, aes(x=locality, y=diff_pct_vote_d))+\n  geom_col()+\n  ggtitle(\"Top 5 Counties with the Largest Difference Between the % \\nof Votes Received by Biden and McAuliffe\")+\n  xlab(\"County\")+\n  ylab(\"% Difference Between Votes for Biden and McAuliffe\")\n\n\n\n\n\n\nExercise 5\nThe five localities with the lowest percent of Trump voters are Petersburg, Charlottesville, Richmond, Falls Church, and Arlington. We can find this quickly using pipe operators. We assign low5_trump to select the columns locality, trump, and trump_pct. We then arrange the data from lowest to smallest value of percent of trump voters with the arrange function. Then we take the top 5 with head() and put it into an interactive table using the DT package. The resulting table is simple because we set the arguements for rownames, searching, and paging as FALSE.\n\nlow5_trump <- va_df %>% \n              select(locality, trump, trump_pct) %>% \n              arrange(trump_pct) %>% \n              head(5) %>% \n              DT::datatable(rownames = FALSE, \n              options = list(searching = FALSE, paging = FALSE, dom = \"tip\")) \n\nlow5_trump\n\n\n\n\n\n\n\n\nExercise 6\nThe five counties with the lowest percent of Biden voters are Tazewell, Scott, Lee, Buchanan, and Bland. We can find this by first making a subset of va_df. First, we select only the locality and biden_pct columns. Then we arrange biden_pct from lowest to highest value. Then we take the first five rows using the head() function. We save this subset to low5_biden. The following code creates a visualization of low5_biden using ggplot notation. ggplot(). ggtitle(), xlab(), and ylab() label different parts of the final graph.geom_text() labels each bar with the corresponding percent of Biden voters.\n\nlow5_biden<- va_df %>% \n             select(locality, biden_pct) %>% \n             arrange(biden_pct) %>%  \n             head(5)\n\nggplot(low5_biden, aes(x=biden_pct, y=locality))+\n  geom_col()+ \n  ggtitle(\"The Five Virginia Counties with the Lowest % of Voters for Biden\")+ \n  xlab(\"% of voters for Biden\") + \n  ylab(\"County\") +\n  geom_text(aes(label = biden_pct))\n\n\n\n\n\n\nExtra Credit\nThe following code creates a function called sort_biden_vote. To use the function you insert a number and it will return the number of counties with the lowest percent of Biden voters. For example, if you typed sort_biden_vote(15), it would return the 15 Virginia counties with the lowest percent of voters for Biden. First, we assign sort_biden_vote to a function that takes one input arguement The function takes the locality, biden, and biden_pct columns from the va_df dataset and orders it by biden_pct from lowest to highest. It then uses head() with the user input and the DT package to return an interactive table that shows the corresponding number of counties with the lowest percent of voters for Biden.\n\nsort_biden_vote <- function(number){\n  biden <-  va_df %>% \n              select(locality, biden, biden_pct) %>% \n              arrange(biden_pct) %>% \n              head(number) %>% \n              DT::datatable(options = list(searching = FALSE, paging = FALSE, dom = \"tip\")) \n    return(biden)\n}\n\nsort_biden_vote(12)"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Who let the dogs out?\n\nA look at how daily dog park outings created a friend group during the COVID-19 pandemic lockdown.\n\n5 things you should know about monkeypox\n\nA listicle sharing different facts and statistics about the 2022 monkeypox outbreak.\n\nWhat the shell: Where did all the oysters go?\n\nA science narrative showcasing the role of eastern oysters in the Chesapeake Bay and my relationship with the species."
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Walkthrough",
    "section": "",
    "text": "Dolphin and Karate Network Analysis\n\nIntroduction\nThe dolphin network comes from a 2003 study in Doubtful Sound, a New Zealand fiord, that analyzed the relationships within a bottlenose dolphin population. Because of the fiord’s geographic isolation, scientists wanted to compare the social organization of Doubtful Sound dolphins to other studied dolphin populations.\nThe karate network graphs the social relationships of students in a university karate club, run by university president, John A. and Mr. Hi. A long-term feud between John A. and Mr. Hi result in the splitting of the club into two separate organizations.\nWhile these networks seemingly have nothing in common, comparing the two allows for a better understanding of network and node characteristics.\n\n\nNetwork Plots\n\n\n\n\n\nAs illustrated by the graphs above, both the dolphin and karate networks are undirected.\nThere are 62 vertexes in the dolphin network, each representing one dolphin in the Doubtful Sound population. The network has 159 edges.\nThere are 34 vertexes in the karate network– one is John A., one is Mr. Hi, and the other 32 are students. The orange nodes are people who joined Mr. Hi’s club after the split and the blue nodes are those who followed John A. The network has 78 edges.\nEven though neither network has multiple components, the nodes of each seem to have clustered into two groups. This is especially highlighted by the colors in the karate network. Based on the visualizations, the dolphin network appears to have more pendants around the outer edge while the karate network has more closed triads.\n\n\nDegree Distribution\n\n\n\n\n\nDegree is the number of connections a node has to other nodes in its network. The histograms above show how many nodes in each network have a certain number of degrees. Both histograms are skewed to the right, indicating that they are real networks. In real networks, there are many nodes with few degrees and few nodes with many degrees. The majority of nodes in the dolphin and karate networks have six degrees or less. There are fewer nodes with higher degrees.\n\n\nTriad Census\nThe triad names follow MAN notation: mutual dyads, asymmetric dyads, and null dyads.Under MAN notation, there are 16 types of triads. Because these are undirected networks, however, we only see four of them. 003 triads have no existing ties between a set of three nodes. 102 triads have one connection between the three nodes. 201 triads have two connections between a set of three nodes. 300 triads are complete triangles.\n\n\n\n\n\n\n\n\nThe dolphin network has 29,108 003 triads, 7,979 102 triads, 638 201 triads, and 95 300 triads. Likewise, the karate network has 3,971 003 triads, 1,575 102 triads, 393 201 triads, and 45 300 triads. 003 triads make up a larger proportion of the dolphin network than they do the karate network. Correspondingly, 102, 201, and 300 triads make up a larger percent of the karate network than they do the dolphin network.\n\n\nDegree Centrality\nA node’s degree centrality is equivalent to its degree. Nodes with high degrees have numerous connections within the network.\n\nDolphin Network Degree Centrality\n\n\n\nNode\nDegree Centrality\nDegree Centrality Normalized\n\n\n\n\nGrin\n12\n0.19672131\n\n\nTopless\n11\n0.18032787\n\n\nSN4\n11\n0.18032787\n\n\nTrigger\n10\n0.16393443\n\n\nScabs\n10\n0.16393443\n\n\n\n\n\nKarate Network Degree Centrality\n\n\n\nNode\nDegree Centrality\nDegree Centrality Normalized\n\n\n\n\nJohn A.\n17\n0.51515152\n\n\nMr. Hi\n16\n0.48484848\n\n\nActor 33\n12\n0.36363636\n\n\nActor 3\n10\n0.30303030\n\n\nActor 2\n9\n0.27272727\n\n\n\n\n\nAverage Degree Centralities\n\n\n\n\n\n\n\n\nNetwork\nAverage Degree Centrality\nAverage Degree Centrality Normalized\n\n\n\n\nDolphin Network\n5.129032\n0.0840825\n\n\nKarate Network\n4.588235\n0.1390374\n\n\n\nIn the dolphin network, the nodes with the highest degrees are Grin, SN4, and Topless. Grin has a degree of 12 and SN4 and Topless have degrees of 11. The average number of degrees a node in the dolphin network has is 5, which normalizes to 0.08. In the karate network, the nodes that have the greatest degree centrality are John A., Mr. Hi, and Actor 33. John A.’s degree centrality is 17, Mr. Hi’s is 16, and Actor 33’s is 12. The average number of degrees a node in the karate network has is 4, which normalizes to 0.13.\nBecause the networks are different sizes, the degree centralities of the two networks can not be directly compared to one another. Looking at the normalized average degree centralities, however, it appears that the average node in the karate network has a larger share of the existing network links than an average node in the dolphin network does.\n\n\n\nBetweenness Centrality\nBetweenness centrality looks at how frequently a node lies on the shortest paths between other vertices. Nodes with high betweenness centrality control the information flow between vertices, meaning they may disrupt a conversation completely.\n\nDolphin Network Betweenness Centrality Table\n\n\n\n\n\n\n\n\nNode\nBetweenness Centrality\nNormalized Betweenness Centrality\n\n\n\n\nSN100\n454.274069\n0.2482371960\n\n\nBeescratch\n390.383717\n0.2133244355\n\n\nSN9\n261.963619\n0.1431495183\n\n\nSN4\n253.582713\n0.1385697887\n\n\nDN63\n216.376673\n0.1182386193\n\n\n\n\n\nKarate Network Betweenness Centrality Table\n\n\n\n\n\n\n\n\nNode\nBetweenness Centrality\nNormalized Betweenness Centrality\n\n\n\n\nMr. Hi\n250.150000\n0.4737689394\n\n\nJohn A.\n209.500000\n0.3967803030\n\n\nActor 20\n127.066667\n0.2406565657\n\n\nActor 32\n66.333333\n0.1256313131\n\n\nActor 33\n38.133333\n0.0722222222\n\n\n\n\n\nAverage Betweenness Centralities\n\n\n\n\n\n\n\n\nNetwork\nAverage Betweenness Centrality\nAverage Betweenness Centrality Normalized\n\n\n\n\nDolphin Network\n71.8871\n0.03928257\n\n\nKarate Network\n26.19363\n0.04960914\n\n\n\nIn the dolphin network, SN100, Beescratch, SN9, SN4, and DN63 have the highest betweenness centralities. Mr. Hi, John A., Actor 20, Actor 32, and Actor 33 have the highest betweenness centralities in the karate network. Mr. Hi and John A. have notably larger betweenness centralities than the other nodes, indicating that these individuals have significant power over the karate network’s information flow. Conversely, betweenness centrality appears more evenly dispersed among the five dolphins with the highest betweenness centralities.\nBased on the normalized betweenness centralities, it seems that a given node in the karate network sits on more of the existing paths in the network, than a given node in the dolphin network does.\n\n\n\nCloseness Centrality\nCloseness centrality describes how easily a node can be reached from other vertices or vice-versa, how easily it can be reached from other nodes. Nodes with high closeness centrality are able to spread information more efficiently through a network.\n\nDolphin Network Closeness Centrality Table\n\n\n\nNode\nCloseness Centrality\nNormalized Closeness Centrality\n\n\n\n\nSN100\n0.006849315\n0.4178082\n\n\nSN9\n0.006622517\n0.4039735\n\n\nSN4\n0.006535948\n0.3986928\n\n\nKringel\n0.006410256\n0.3910256\n\n\nGrin\n0.006172840\n0.3765432\n\n\n\n\n\nKarate Network Closeness Centrality Table\n\n\n\nNode\nCloseness Centrality\nNormalized Closeness Centrality\n\n\n\n\nMr. Hi\n0.007692308\n0.2538462\n\n\nJohn A.\n0.007633588\n0.2519084\n\n\nActor 20\n0.007518797\n0.2481203\n\n\nActor 32\n0.006329114\n0.2088608\n\n\nActor 13\n0.006211180\n0.2049689\n\n\n\n\n\nAverage Closeness Centralities\n\n\n\n\n\n\n\n\nNetwork\nAverage Closeness Centrality\nAverage Closeness Centrality Normalized\n\n\n\n\nDolphin Network\n0.005036721\n0.30724\n\n\nKarate Network\n0.005450958\n0.1798816\n\n\n\nSN100, SN9, SN4, Kringel, and Grin are the dolphins with the highest closeness centralities. In the karate network, Mr. Hi, John A., Actor 20, Actor 32, and Actor 13 have the highest closeness centralities.\nUsing the normalized average closeness centrality, the average dolphin is able to spread information more efficiently through its network than the average actor would be able to spread it in the karate network.\n\n\n\nEigenvector Centrality\nA node’s eigenvector centrality is based on the centrality of its neighbors. A high eigenvector centrality means a node is influential in the network because it is connected to other nodes with high centralities.\n\nDolphin Network Eigenvector Centrality\n\n\n\nNode\nEigenvector Centrality\n\n\n\n\nGrin\n1.000000000\n\n\nSN4\n0.951799805\n\n\nTopless\n0.902535447\n\n\nScabs\n0.890164531\n\n\nTR99\n0.689371799\n\n\n\n\n\nKarate Network Eigenvector Centrality\n\n\n\nNode\nEigenvector Centrality\n\n\n\n\nJohn A.\n1.00000000\n\n\nActor 3\n0.99036448\n\n\nActor 33\n0.91256318\n\n\nMr. Hi\n0.85787944\n\n\nActor 2\n0.82876616\n\n\n\n\n\nAverage Eigenvector Centralities\n\n\n\nNetwork\nAverage Eigenvector Centrality\n\n\n\n\nDolphin Network\n0.2874554\n\n\nKarate Network\n0.3772814\n\n\n\nIn the dolphin network, Grin, SN4, Topless, Scabs, and TR99 have the highest eigenvector centralities. In the karate network, John A., Actor 3, Actor 33, Mr. Hi, and Actor 2 have the highest eigenvector centralities. Both Grin and John A. have eigenvector centralities of 1. Something interesting to note is that Mr. Hi has neither the first or second highest eigenvector centrality in the network.\nComparing the networks by average eigenvector centralities, it seems that a given person in the karate network would have more influence in its network than a dolphin would have in its own.\n\n\n\nCentrality Correlation\n\nDolphin Network Centrality Correlations\n\n\n           d_degree d_between   d_close\nd_degree  1.0000000 0.5902140 0.7126718\nd_between 0.5902140 1.0000000 0.6657346\nd_close   0.7126718 0.6657346 1.0000000\n\n\nThe dolphin network centralities are all positively correlated. Closeness and degree are the most correlated, followed by closeness and betweenness, and then betweenness and degree.\n\n\nKarate Network Centrality Correlations\n\n\n           k_degree k_between   k_close\nk_degree  1.0000000 0.7882018 0.5860469\nk_between 0.7882018 1.0000000 0.7119957\nk_close   0.5860469 0.7119957 1.0000000\n\n\nThe karate network centralities are all positively correlated. Betweenness and degree have the highest correlation, followed by closeness and betweenness, and then closeness and degree.\n\n\n\nConclusion\nHere are some of the key takeaways from this analysis and comparison of the dolphin and karate networks:\n\nDolphin Network Analysis\n\nSN4, Grin, and SN100 are critical nodes in the dolphin network. They have high degrees and centralities, meaning they are well integrated into the network and connect many of the other dolphins to each other.\nBased on the large number of 003 triads in the census, it seems that clustering is limited in the dolphin network. While there are no isolates in the network, 9 of the dolphins have only have a degree of one. They do not contribute much to the overall structure of the network.\n\nKarate Club Network Analysis\n\nWhile there is some overlap between the clubs, the network clearly illustrates tension between Mr. Hi and John A. Network shape and color show two distinct factions.\nBased on the betweenness and closeness centrality, Mr. Hi controls a lot of the information flow, the speed of it and who it reaches. Despite this, he does not have much influence as other actors because of his lower eigenvector centrality.\n\nComparison of the Dolphin and Karate Club\n\nHigh centralities are spread more evenly across dolphin nodes than in the karate nodes. In the karate network, Mr. Hi and John A. hold the most power and are consistently in the five highest centralities.\nAny tension existing in the dolphin network is less noticeable. Its structure appears more fluid than the divided karate network.\nThe differences between centrality correlations indicate structural differences too. In the karate network, nodes with high degrees appear to intersect other communication pathways more than nodes in the dolphin network. In the dolphin network, if you have a high degree you’re easier to reach than similar nodes in the karate network.\nAll of the differences make sense because the dolphin network data comes from observations in the natural environment. Meanwhile, the karate network data was designed to highlight friction in an organization that eventually split into two groups.\n\n\nIt would be interesting to take a closer look at the networks individually through metrics like, cluster coefficient and density.\n\n\nAnalysis Code\nHere is the code used to create this analysis. For viewing ease, lines of code that plot graphs have been included as comments (i.e. #plot(dolphin_network, main=“Dolphin Network”)). To reciprocate analysis, remove the #s and run the code for plot(), hist(), and ggplot().\nNote that normalizing a measurement refers to the process of setting it to a scale to make the numbers comparable to one another.\n\n\nCode\n#The following three lines of code install the necessary library packages for social network analysis. \nlibrary(igraph)\nlibrary(igraphdata)\nlibrary(ggplot2)\n\n#The next two lines of code load the datasets and assign them to the variables, dolphin_df and karate_df.\ndolphin_df <- \"https://raw.githubusercontent.com/graphstream/gs-gephi/master/data/dolphins.gml\"\nkarate_df <- data(\"karate\")\n#This reads the data set to create a graph object of the dolphin network. \ndolphin_network <- read_graph(dolphin_df, format=\"gml\")\n#This prints all of the attribute data for the dolphin network, including dolphin name and ID number, using get.vertex.attribute().\nget.vertex.attribute(dolphin_network)\n#This pulls a list of all of the connections in the dolphin network, using E(). \nE(dolphin_network)\n#This reads the data set to create a graph object of the karate network. \nkarate_network <- karate\n#This prints all of the attribute data for the karate network, including node name, label number, faction, and corresponding color, , using get.vertex.attribute(). \nget.vertex.attribute(karate_network)\n#This pulls a list of all of the connections in the karate network, using E(). \nE(karate_network)\n\n#The following three lines graph the networks next to each other. par(mfrow=c(1,2)) creates a matrix for the plots to be printed in. main=\"\" sets the graph's title.\n#par(mfrow=c(1,2))\n#plot(dolphin_network, main=\"Dolphin Network\")\n#plot(karate_network, main=\"Karate Network\")\n\n#The following three lines graph the histograms of the networks' degree distributions next to each other. par(mfrow=c(1,2)) creates a matrix for the plots to be printed in. main=\"\" sets the graph's title.\n#par(mfrow=c(1,2))\n#hist(degree(dolphin_network), main=\"Dolphin Network \\nDegree Distribution\")\n#hist(degree(karate_network), main=\"Karate Network \\nDegree Distribution\")\n\n#This creates a new data frame that includes triad information about the dolphin network. triad.census() analyzes a network's triad structure. colnames() sets the column names in the new data frame created with as.data.frame. \ndolphin_triad <- as.data.frame(triad.census(dolphin_network))\ndolphin_triad$code <- c(\"003\", \"012\", \"102\", \"021D\", \"021U\", \"021C\", \"111D\", \"111U\",\n             \"030T\", \"030C\", \"201\", \"120D\", \"120U\", \"120C\", \"210\", \"300\")\ncolnames(dolphin_triad) <- c(\"freq\", \"code\")\n\n#This creates a new data frame that includes triad information about the karate network. triad.census() analyzes a network's triad structure.colnames() sets the column names in the new data frame created with as.data.frame.\nkarate_triad <- as.data.frame(triad.census(karate_network))\nkarate_triad$code <- c(\"003\", \"012\", \"102\", \"021D\", \"021U\", \"021C\", \"111D\", \"111U\",\n             \"030T\", \"030C\", \"201\", \"120D\", \"120U\", \"120C\", \"210\", \"300\")\ncolnames(karate_triad) <- c(\"freq\", \"code\")\n\n#This graphs the distribution of triad structures in the dolphin network with ggplot() notation. It is a bar graph, filled with blue columns, with the x-axis label \"Triad type based on MAN notation\" and the y-axis label \"Count\" The graph is titled \"Dolphin Network Triad Census\". \n#ggplot(dolphin_triad, aes(x = code, y = freq)) +\n  #geom_bar(fill = \"#0073C2FF\", stat = \"identity\") +\n  #geom_text(aes(label = freq), vjust = -0.3) +\n  #theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  #xlab(\"Triad type based on MAN notation\") +\n  #ylab(\"Count\") +\n  #ggtitle(\"Dolphin Network Triad Census\")\n\n#This graphs the distribution of triad structures in the karate network  with ggplot() notation. It is a bar graph, filled with blue columns, with the x-axis label \"Triad type based on MAN notation\" and the y-axis label \"Count\" The graph is titled \"Karate Network Triad Census\". \n#ggplot(karate_triad, aes(x = code, y = freq)) +\n  #geom_bar(fill = \"#0073C2FF\", stat = \"identity\") +\n  #geom_text(aes(label = freq), vjust = -0.3) +\n  #theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  #xlab(\"Triad type based on MAN notation\") +\n  #ylab(\"Count\") +\n  #ggtitle(\"Karate Network Triad Census\")\n\n#The next chunk of code focuses on pulling information about nodes' degree centrality using the function degree(). Variations of the basic function are made by adding the argument normalized=TRUE. mean() calculates the average.  \n#This is the average degree of nodes in the dolphin network. \nmean(degree(dolphin_network))\n#This pulls the degree of every node in the dolphin network, sorting in order from lowest to highest, and assigning their names. \nsort(setNames( degree(dolphin_network),V(dolphin_network)$label))\n#This pulls the normalized degree of every node in the dolphin network, sorting in order from lowest to highest, and assigning their names. \nsort(setNames( degree(dolphin_network, normalized = TRUE),V(dolphin_network)$label))\n#This is the normalized average degree of nodes in the dolphin network. \nmean(degree(dolphin_network, normalized=TRUE))\n#This is the average degree of nodes in the karate network. \nmean(degree(karate_network))\n#This pulls the degree of every node in the karate network, sorting in order from lowest to highest.\nsort(degree(karate_network))\n#This pulls the normalized degree of every node in the karate network, sorting in order from lowest to highest.\nsort(degree(karate_network, normalized = TRUE))\n#This is the normalized average degree of nodes in the karate network.\nmean(degree(karate_network, normalized=TRUE))\n\n\n#The next chunk of code focuses on pulling information about nodes' betweenness centrality using the function betweenness(). Variations of the basic function are made by adding the argument normalized=TRUE. mean() calculates the average.  \n#This is the average betweenness of a node in the dolphin network. \nmean(betweenness(dolphin_network))\n#This calculates the betweenness of every node in the dolphin network, sorting in order from lowest to highest, and assigning their names. \nsort(setNames( betweenness(dolphin_network),V(dolphin_network)$label))\n#This calculates the normalized betweenness of every node in the dolphin network, sorting in order from lowest to highest, and assigning their names. \nsort(setNames( betweenness(dolphin_network, normalized=TRUE),V(dolphin_network)$label))\n#This is the normalized average betweeenness of a node in the dolphin network. \nmean(betweenness(dolphin_network, normalized = TRUE))\n#This is the average betweenness of a node in the karate network. \nmean(betweenness(karate_network))\n#This calculates the betweenness of every node in the karate network, sorting in order from lowest to highest.\nsort(betweenness(karate_network))\n#This calculates the normalized betweenness of every node in the karate network, sorting in order from lowest to highest.\nsort(betweenness(karate_network, normalized=TRUE))\n#This is the normalized average betweenness of a node in the karate network. \nmean(betweenness(karate_network, normalized = TRUE))\n\n#The next chunk of code focuses on pulling information about nodes' closeness centrality using the function closeness(). Variations of the basic function are made by adding the arguement normalized=TRUE. mean() calculates the average.  \n#This is the average closeness of a node in the dolphin network. \nmean(closeness(dolphin_network))\n#This sorts the closeness centralities of each node in the dolphin network from lowest to highest with their names. \nsort(setNames( closeness(dolphin_network),V(dolphin_network)$label))\n#This sorts the normalized closeness centralities of each node in the dolphin network from lowest to highest with their names. \nsort(setNames( closeness(dolphin_network, normalized=TRUE),V(dolphin_network)$label))\n#This is the normalized average closeness of a node in the dolphin network.\nmean(closeness(dolphin_network, normalized = TRUE))\n#This is the average closeness of a node in the karate network. \nmean(closeness(karate_network))\n#This sorts the closeness centralities of each node in the karate network from lowest to highest.\nsort(closeness(karate_network))\n#This sorts the normalized closeness centralities of each node in the karate network from lowest to highest.\nsort(closeness(karate_network, normalized=TRUE))\n#This is the normalized average closeness of a node in the karate network. \nmean(closeness(karate_network, normalized = TRUE))\n\n#The next chunk of code focuses on pulling information about nodes' eigenvector centrality using the function eigen_centrality(). Variations of the basic function are made by adding the arguement normalized=TRUE. mean() calculates the average.  \n#This pulls the eigenvector centrality for each node in the dolphin network.\ndolphin_eigen <- eigen_centrality(dolphin_network)$vector\n#This sorts the dolphin network eigenvector centralities from lowest to highest with their names. \nsort(setNames(dolphin_eigen, V(dolphin_network)$label))\n#This is the average eigenvector centrality for nodes in the dolphin network. \nmean(eigen_centrality(dolphin_network)$vector)\n#This sorts and calculates the karate network eigenvector centralities from lowest to highest. \nsort(eigen_centrality(karate_network)$vector)\n#This is the average eigenvector centrality for nodes in the karate network. \nmean(eigen_centrality(karate_network)$vector)\n\n#This saves the dolphin network degree centralities to d_degree.\nd_degree <-  degree(dolphin_network)\n#This saves the dolphin network betweenness centralities to d_between.\nd_between <- betweenness(dolphin_network)\n#This saves the dolphin network closeness centralities to d_close.  \nd_close <- closeness(dolphin_network)\n#This creates a data frame of the dolphin network's nodes' centrality measurements by using data.frame() and inserting the saved centralities. \nd_df <- data.frame(d_degree, d_between, d_close)\n#This creates a correlation matrix for the different centralities of the dolphin network using cor(). \ncor(d_df)\n\n#This saves the karate network degree centralities to k_degree.\nk_degree <-  degree(karate_network)\n#This saves the karate network betweenness centralities to k_between.\nk_between <- betweenness(karate_network)\n#This saves the karate network closeness centralities to k_close. \nk_close <- closeness(karate_network)\n#This creates a data frame of the karate network's nodes' centrality measurements by using data.frame() and inserting the saved centralities.\nk_df <- data.frame(k_degree, k_between, k_close)\n#This creates a correlation martrix for the different centralities of the karate network using cor(). \ncor(k_df)"
  }
]